% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tokenize.r
\name{tokenize_document}
\alias{tokenize_document}
\title{Tokenize a document}
\usage{
tokenize_document(x, pattern, stopwords = NULL, strip_punct = TRUE,
  stem_language = NULL)
}
\arguments{
\item{x}{list of documents}

\item{pattern}{regex pattern to generate tokens from}

\item{stopwords}{vector of words to remove}

\item{strip_punct}{whether or not to strip single punctuation tokens}

\item{stem_language}{which language to use (if any) for stemming using \code{\link[SnowballC]{wordStem}}}
}
\description{
Tokenize a document
}
